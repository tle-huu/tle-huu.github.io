

<div style="text-align: center"> 
"On the highway towards Human-Level AI, Large Language Model is an off-ramp."<br>
Why is it fundamentally flawed?
</div>

<br>
[Download pdf](/files/we_are_missing_something_collectively.pdf)

<br>
<div style="text-align: justify"> 

This small text is a spontaneous (but hopefully constructive) discussion of Yann Le Cun's vision about the way to Human-Level AI, or Autonomous Machine Intelligence. I have quickly written these notes right after reading Yann Le Cun's <a href="https://openreview.net/forum?id=BZ5a1r-kVsf"> <u>A Path Towards Autonomous Machine Intelligence, Version 0.9.2</u></a>.  
</div>
<br>

My thesis can be summarized as follows: 

- Language is inherent to Learning 
- Learning is a collective behavior 
- Human-Level AIs will be collective or not be 


<div style="text-align: justify"> 
At  a  time  when  everyone  claims  that  ChatGPT  is  a  breathtaking  step  to  AGI, Yann  Le  Cun encourages (urges?) to caution and humbleness. And I think he cannot be more right. 

<br><br>
Those notes are spontaneous thoughts, written with the feeling that we are missing something when dealing with learning. Most of the points need deeper development, but the hope is that they would be a good start and food for thoughts on what we are trying to achieve and why. They would fit in the “Discussions, limitations” section of the paper. 

<br><br>
In the paper, Yann Le Cun develops his vision on the path toward Human-Level AI. He starts by explaining how human beings learn and how we should devise machines in a similar way, before going into the (more) technical implementation details. What ignited my curiosity is twofold: Learning is defined as an individual concept, and the “negligence” for the Language. 

<br><br>
I was first piqued in the introduction by a seemingly benign sentence that invites "impatient readers [...] to jump directly" to the sections of interest, and regret this introduction section was not more filled out. 

<br><br>
In my opinion, any paper or text that has the ambition of developing a path, as technical as it may be, toward Human-Level AI, must take a significant amount of time defining what a "Human- Level", “Learning”, or “Intelligence” mean. This is not secondary. This is the Archimedean point on which everything will be built upon. It is worthless trying to develop a theory in a broken framework, because it would not be able to overtake the limitation of the framework in which it lives in, by construction. I therefore recommend reading the introduction of the paper with great attention, as well as the related work and discussion sections. 

<br><br>
I submit that the very fundamental philosophical idea on which the paper is based on is flawed by  nature  because  of  a  (collective)  misconception  of  Learning  and  more  generally  human interactions.  This  flaw  comes  from  our  individualistic  and  anthropocentric  model  of  the humankind. (Please read “flawed” as “incomplete”, “imperfect”, not “wrong”).  

<br><br>
We are focusing on the individual agent, instead of embracing the collective intelligence, “the big picture”. 

<br><br>
To be very clear, my goal is not to discuss the paper itself, but to rather challenge our tech community. I am afraid that we might be confining ourselves in a tunnel that it may be extremely difficult to get out from. 

<br><br>
The following text thus focuses on the Introduction of the paper. The rest of the paper (what most readers will be interested in) is not discussed.

<br><br>
</div>

[Download text](/files/we_are_missing_something_collectively.pdf)
